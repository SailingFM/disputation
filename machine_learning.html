<!doctype html>
<html lang="en">
  <head>
    <link href="captionss/captionss.css" rel="stylesheet" type="text/css">
    <meta charset="utf-8">
    <title>6DoF Pose using Neural Networks</title>
    <meta name="author" content="Jérémie Papon">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="reveal/css/reveal.min.css">
    <link rel="stylesheet" href="pkgwtheme.css" id="theme">
    <link rel="stylesheet" href="reveal/lib/css/zenburn.css">
    <script>
      document.write('<link rel="stylesheet" href="reveal/css/print/'+(window.location.search.match(/print-pdf/gi) ? 'pdf' : 'paper')+'.css" type="text/css" media="print">');
    </script>
    <!--[if lt IE 9]>
    <script src="reveal/lib/js/html5shiv.js"></script>
    <![endif]-->
  
  </head>

  <body>
    <div class="reveal">
      <resetfootnotecounter></resetfootnotecounter>
      <resetcitationcounter></resetcitationcounter>

      <div class="slides">
        <section>
          <p style="text-align: center"><br><br><br>Turn off the popup blocker before starting!</p>
          <p style="text-align: center">Open presenter console with <code>s</code>.</p>
          <p style="text-align: center">This really isn’t going to work if the assets haven't been downloaded!</p>
        </section>

        
        

        <!-- 
        +++++++++++++++++++++
        MACHINE LEARNING STUFF
        +++++++++++++++++++++
        -->
        <section>
          <h2> Current Work - Estimating Pose using deep CNNs </h2>
          <p> Generic 6DoF Pose Estimation is a <em>hard</em> problem </p>
          <ul>
            <li class="fragment">Baseline methods work by finding feature or point correspondences (e.g. ICP)</li>
            <li class="fragment">Much effort has been spent designing model-based approaches for particular classes, but this is time consuming.</li>
          </ul>
          <figure class="embed hide-smooth dark" >
                  <img src="data/bunny.png" >
                  <figcaption style="font-size:1.0em">
                    Can we avoid needing to design features for correspondences, and learn them instead?
                  </figcaption>
          </figure> <br>
          <p class="fragment">Unfortunately, generic pose is very difficult to get training data for.</p>
          <p class="fragment" style="font-size:150%; text-align:center">Fortunately, we can synthesize it!</p>
        </section>
        
        <section>
          <h2>Synthetic Dataset</h2>
          <ul>
            <li class="fragment">Generate data with random poses so we can have accurate ground truth</li>
            <li class="fragment">Use meshes from Princeton ModelNet <cite></cite> - for now, chairs</li>
            <br>
            <li class="fragment">Why Chairs?</li>
            <div class="fragment" >
              <ul style="list-style-type: none">
                <li>Lots of them, aligned with canonical pose</li>
                <li>Can limit degrees of freedom initially</li>
                <li>Lots of intra-class variability, but can still generalize</li>
              </ul>
              <figure class="embed-bottom reveal-smooth dark" >
                <div class="ctr w100">
                    <img src="data/pose/chairs.png">
                    <figcaption style="font-size:1.0em">
                        Chairs. Lots of Chairs.
                    </figcaption>
                </div>
              </figure>
            </div>
          </ul>
          <div class='citation'>
              <footl>
                <footi>Wu, et al. <a href="http://arxiv.org/abs/1406.5670">"3D ShapeNets for 2.5D Object Recognition and Next-Best-View Prediction"</a> <em> arXiv </em> 2014.</footi>
              </footl>
          </div>
        </section>
        
        <section>
            <h2>Synthetic Data</h2>
            <p> Generate data using Blensor Python API and a custom Kinect Sensor Model</p>
            <p> Basic "Room" with 7 randomly (not touching) placed chairs.</p>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/blender.png">
              </div>
            </figure>
        </section>
        
        <section>
          <section>
            <h2>Synthetic Data - Depth</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_depth.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic Data - Depth</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/1_depth.png">
              </div>
            </figure>
          </section>
        </section>
        
        <section>
          <section>
            <h2>Synthetic Data - Normals</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_nx.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic Data - Normals</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_ny.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic Data - Normals</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_nz.png">
              </div>
            </figure>
          </section>
        </section>
        
        <section>
          <section>
            <h2>Synthetic Data - Labels</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_labels.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic Data - Labels</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/1_labels.png">
              </div>
            </figure>
          </section>
        </section>
        
        <section>
          <section>
            <h2>Synthetic vs Real</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_depth.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic vs Real</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/real_depth.png">
              </div>
            </figure>
          </section>
          
          <section>
            <h2>Synthetic vs Real</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_nx.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic vs Real</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/real_nx.png">
              </div>
            </figure>
          </section>
        </section>
        
        <section>
          <section>
            <h2>Signatures - Depth</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/depth_sigs.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Signatures - nx</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/nx_sigs.png">
              </div>
            </figure>
          </section>
          
          <section>
            <h2>Signatures - ny</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/ny_sigs.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Signatures - nz</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/nz_sigs.png">
              </div>
            </figure>
          </section>
        </section>

        <section>
          <section>
            <h2>Signatures Zero padded - Depth</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/example_depth.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Signatures - nz</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/example_nz.png">
              </div>
            </figure>
          </section>
        </section>

        <section>
          <section>
            <h2>Signatures Zero padded - Depth</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/zeropad_sigs_depth.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Signatures - nx</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/zeropad_sigs_nx.png">
              </div>
            </figure>
          </section>
          
          <section>
            <h2>Signatures - ny</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/zeropad_sigs_ny.png">
              </div>
            </figure>
          </section>
          <section>
            <h2>Signatures - nz</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/zeropad_sigs_nz.png">
              </div>
            </figure>
          </section>
        </section>


        
        <section>
          <section>
            <h2>LCCP to Generate Proposals</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_lccp_boxes.svg">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic Data - Labels</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/0_labels.png">
              </div>
            </figure>
          </section>
          
          <section>
            <h2>LCCP to Generate Proposals</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/1_lccp_boxes.svg">
              </div>
            </figure>
          </section>
          <section>
            <h2>Synthetic Data - Labels</h2>
            <figure class="embed-top reveal-smooth dark" >
              <div class="ctr w80">
                  <img src="data/pose/1_labels.png">
              </div>
            </figure>
          </section>
        </section>
        
         <section>
            <h2>Network Architecture</h2>
            <p> Pretty standard configuration except for input layer, a la Krizhevsky Imagenet<cite></cite>
            <figure class="embed-top reveal-smooth dark" >
               <div class="ctr w80">
                  <img src="data/pose/Architecture.png">
                  <figcaption style="font-size:1.0em">
                      Network Architecture - 3 Conv. layers, 2 fully connected. 
                  </figcaption>
              </div>
            </figure><br>
            <div class='citation'>
              <footl>
                <footi>Krizhevsky, et al. <a href="http://papers.nips.cc/paper/4824-imagenet">"Imagenet classification with deep convolutional neural networks."</a> <em> NIPS </em> 2012.</footi>
              </footl>
            </div>
        </section>
        
        <section>
            <h2>Loss Function</h2>
            <p> MSE of transformed model points (a la ICP) </p>
            <p>$$ L_i = \frac{1}{ \sum{w_{i,j} } } \sum_{i=1}^{N_o} \sum_{j=1}^{N_m} w_{i,j} \lVert o_i-(\theta m_i + dp) \rVert ^2$$</p>
            <ul>
              <li class="fragment" style="color:green">Direct measure - simply distance between points</li>
              <li class="fragment" style="color:red">Expensive to compute!</li>
              <li class="fragment" style="color:red">Unreliable with occlusions, partial views have local minima, not convex</li>
            </ul>
            <p> MSE of pose directly - euler angles, translation </p>
            <p>$$ L_i = \lVert f - \mathbf{y_i} \rVert ^2$$</p>
             <ul>
              <li class="fragment" style="color:green">Cheap to compute</li>
              <li class="fragment" style="color:green">Will give pose under heavy occlusion <span style="color:red">but no measure of reliability</span></li>
              <li class="fragment" style="color:green">Generalizes well for classes with canonical pose (i.e. most)</li>
              <li class="fragment" style="color:red">Possible scale issues between rotational and translational errors</li>
              <li class="fragment" style="color:red">Regression is harder, less stable than classification</li>
            </ul>
            <p> Binned Pose Binary Log. Regression Classification</p>
            <ul>
              <li class="fragment">CNN predicts a score for a set of bins over the pose space</li>
              <li class="fragment" style="color:green">More stable than regression, easier to train</li>
              <li class="fragment" style="color:red">Only produces coarse pose</li>
            </ul>
            <p>$$ L_i = \sum_j y_{ij} \log(\sigma(f_j)) + (1 - y_{ij}) \log(1 - \sigma(f_j)) $$</p>

            <p> Projected alignment? Other possibilities? </p>
        </section>
       
        <section>
          <section>
              <h2>Early Results</h2>
              <p> Minimal validation error = 0.017175
              <figure class="embed-top reveal-smooth dark" >
                <div class="ctr w80">
                    <img src="data/pose/train_valid_loss.png">
                </div>
              </figure>
          </section>
          <section>
              <h2>Early Results - First Conv layer filters</h2>
              <figure class="embed-top reveal-smooth dark" >
                <div class="ctr w80">
                    <img src="data/pose/responses1.png">
                </div>
              </figure>
          </section>
        </section>
       
        
        


    <!-- End of slides. -->

    <script src="reveal/lib/js/head.min.js"></script>
    <script src="reveal/js/reveal.min.js"></script>
    <script src="pdfjs/compatibility.js"></script>
    <script src="pdfjs/pdf.js"></script>

    <script>
      Reveal.initialize({
        width: 1200,
        height: 800,
        controls: true,
        progress: true,
        history: true,
        center: true,
        keyboard: true,
        overview: true,
        
        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'default', // none/fade/slide/convex/concave/zoom
        transitionSpeed: 'default', // default/fast/slow

        math: {
          mathjax: 'mathjax/MathJax.js',
          config: 'TeX-AMS_HTML-full',
        },

        dependencies: [
          { src: 'reveal/lib/js/classList.js',
            condition: function() { return !document.body.classList; }},
          { src: 'reveal/plugin/markdown/marked.js',
            condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal/plugin/markdown/markdown.js',
            condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal/plugin/highlight/highlight.js', async: true,
            callback: function() { hljs.initHighlightingOnLoad (); }},
          { src: 'reveal/plugin/notes/notes.js', async: true,
            condition: function() { return !!document.body.classList; }},
          { src: 'mymath.js', async: true },
          { src: 'pdfimgs.js', async: true },
          { src: 'slideautostart.js', async: true },
        ],
      });

      // Only load SDO Data page if we go to that slide
      Reveal.addEventListener ('sdodataslide', function () {
        console.log('sdodataslide');
        document.querySelector ('#sdoiframe').src = 'http://sdo.gsfc.nasa.gov/data/'
      });
    </script>
  </body>
</html>
