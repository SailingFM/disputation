<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Perceptual Segmentation of Visual Streams by Tracking of Objects and Parts</title>
    <meta name="author" content="Jérémie Papon">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="reveal/css/reveal.min.css">
    <link rel="stylesheet" href="pkgwtheme.css" id="theme">
    <link rel="stylesheet" href="reveal/lib/css/zenburn.css">
    <script>
      document.write('<link rel="stylesheet" href="reveal/css/print/'+(window.location.search.match(/print-pdf/gi) ? 'pdf' : 'paper')+'.css" type="text/css" media="print">');
    </script>
    <!--[if lt IE 9]>
    <script src="reveal/lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <!-- Slides! -->

    <div class="reveal">
      <div class="slides">
        <section>
          <p style="text-align: center"><br><br><br>Turn off the popup blocker before starting!</p>
          <p style="text-align: center">Open presenter console with <code>s</code>.</p>
          <p style="text-align: center">This really isn’t going to work if the assets haven't been downloaded!</p>
        </section>

        
        <section data-background="data/Buildings_Cropped_Merge.png">
          <h1>Perceptual Segmentation of Visual Streams</h1>
          <h3>by Tracking of Objects and Parts</h3>
          <footer style="font-size: 36px">             
          <a href="http://www.jeremiepapon.com/">Jérémie Papon</a><br>
          Georg-August-Universität Göttingen <br>
          Institut für Informatik <br>
          Göttingen, 2014 Oct 17
          </footer>
          <br>
          <p style="text-align: center"> Disputation for the award of the degree "Doctor of Philosophy" </p>
          
        </section>

        <section>
          <h2>How do we learn to perceive objects?</h2>
          <blockquote>“Infants appear to perceive objects by analyzing three-dimensional surface arrangements and motions... [they] divide perceptual arrays into units that move as connected wholes, that move separately from one another, that tend to maintain their size and shape over motion, and that tend to act upon each other only on contact.” *</blockquote>
          <div class="fragment">
            <div class="w50" style="display: inline-block">
            <p>There are multiple interacting elements essential to development:</p>
              <ul>
                <li class="fragment">Coherent motion at multiple levels</li>
                <li class="fragment">Temporal continuity of size and shape</li>
                <li class="fragment">Only interact with contact</li>
              </ul>
            </div>
            <div class="w45" style="float: right">
              <img src="data/child_colors.jpg">
                
            </div>
          </div>
           <div class='footer'>
              * Spelke, Elizabeth S. "Principles of object perception." Cognitive science 14, no. 1 (1990): 29-56.
            </div>
        </section>  
        
        <section>
          <h2>Observations have no semantic meaning.</h2>
          <p>How can we create meaningful spatial and temporal partitioning of discrete sensor observations?</p>
          <video src="assets/sdo-plasma-rain-yt720.webm" width="100%" muted
                 controls loop class="slideautostart"></video>
          <p class="lcred">NASA Solar Dynamics Observatory AIA
          — <a href="http://youtu.be/M4kT0xoCiPg">YouTube</a></p>
        </section>

        
        <section>
          <h2>Octree Voxelization</h2>
          <div class="w45" style="float: right">
            <img src="data/bunnywork.png">
            <p class="rcred"><a href="http://www.pointclouds.org/">Point Cloud Library (PCL)</a></p>
          </div>
          <p>Insert pointcloud into a grid of cubic voxels.</p>
          <p>Represent all points in one cell by its centroid.</p>
          <p>$$\vec{ \overline{p}} = \frac{1}{N_i}\sum_i \vec p_i$$</p>
          <p>The edge length $L_\text{voxel}$ of voxels defines scale of observation and determines octree minimum bin size.</p>
        </section>
        
        <section>
          <h2>Building an Adjacency Graph</h2>
            <ul>
              <li class="fragment">Special octree type developed which maintains adjacency information of voxels</li>
              <li class="fragment">This gives us back pixel-like (grid) relations, while keeping real 3D adjacency</li>
              <li class="fragment">Region growing and connectivity graph become very efficient</li>
            </ul>
          <div class="ctr w90">
            <img src="assets/wp-Sun_poster.svg">
            <p class="lcred"><a href="http://commons.wikimedia.org/wiki/File:Sun_poster.svg">Wikimedia Commons</a></p>
          </div>
        </section>
        
        <section>
          <h2>Voxel Cloud Connectivity Segmentation</h2>
          <div class="ctr w100">
            <img src="data/supervoxelwhite.svg" width="75%">
          </div>
          <ul>
            <li class="fragment">VCCS is an oversegmentation technique that uses local geometry to respect object boundaries</li>
            <li class="fragment">Constrained to flow across voxel connections</li>
            <li class="fragment">Use color, normals, and a spatial smoothness constraint </li>
          </ul>
          <div class="ctr w100">
            <img src="data/Supervoxels4.svg" width="80%">
          </div>
          <div class='footer'>
               Papon et al. <em>Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds</em>. CVPR, 2013
           </div>
        </section>
  
        <section>
          <h2>Examples of Supervoxels</h2>
          <div class="ctr w100">
            <img src="data/IncreasingSeedSizePlain.svg" width="100%" >
            <p class="rcred"><a href="http://www.jeremiepapon.com/cvpr-2013-supervoxels/">Papon et al. CVPR 2013</a></p>
          </div>
          <p> Example of Supervoxels with different seed sizes - from NYU Dataset * </p>
          <p> Seeding size determines level of detail captured.</p>
          <div class='footer'>
              * Silberman et al., <em>Indoor Segmentation and Support Inference from RGBD Images,</em> ECCV 2012.
          </div>
        </section>
        
        
        
        
        
        <section>
          <h2>Summary</h2>
        </section>
        
        <section>
          <h2>Outlook and Future Work</h2>
        </section>
        
        <section>
          <h2>Acknowledgements and Thanks</h2>
          <h4 align="left">Thesis-related Publications</h4>
          <ul style="font-size: 60%; line-height: 135%; width:50%; float:left">
            <li><strong>Papon, J.</strong>;  Wörgötter, F., <a href="http://www.jeremiepapon.com/wacv-2015-tracking/">Spatially Stratified Correspondence Sampling for Real-Time Point Cloud Tracking,</a> Applications of Computer Vision (WACV), 2015 IEEE International Conference on, Jan. 2015.</li>
            <br style="line-height:150%">
            <li>Stein, S.; Schoeler, M.; <strong>Papon, J.</strong>;  Wörgötter, F., <a href="http://www.jeremiepapon.com/cvpr-2014-segmentation/">Object Partitioning using Local Convexity,</a> Computer Vision and Pattern Recognition (CVPR) 2014, June 2014. </li>
            <br style="line-height:150%">
            <li><strong>Papon, J.</strong>;  Kulvicius, T.; Aksoy, E.; Wörgötter, F. <a href="http://www.jeremiepapon.com/iros-2013-video-segmentation/">Point Cloud Video Object Segmentation using a Persistent Supervoxel World-Model,</a> Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on, Nov. 2013.</li>
            <br style="line-height:150%">
            <li><strong>Papon, J.</strong>; Abramov, A.; Schoeler, M.; Wörgötter, F., <a href="http://www.jeremiepapon.com/cvpr-2013-supervoxels/">Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds,</a> Computer Vision and Pattern Recognition (CVPR) 2013, June 2013.</li>
            <br style="line-height:150%">
            <li><strong>Papon, J.</strong>; Abramov, A.; Wörgötter, F., <a href="http://www.jeremiepapon.com/eccv-2012-ws-2d-video-segmentation/">Occlusion Handling in Video Segmentation via Predictive Feedback,</a> European Conference on Computer Vision (ECCV) 2012, Workshops and Demonstrations, Oct. 2012.</li>
            <br style="line-height:150%">
            <li><strong>Papon, J.</strong>; Abramov, A.; Aksoy, E.; Wörgötter, F., <a href="http://www.jeremiepapon.com/wacv-2012-oculus-system/">A modular system architecture for online parallel vision pipelines,</a> Applications of Computer Vision (WACV) 2012, Jan. 2012.</li>
          </ul>
        </section>
        
        <section>
          <h2>Left Column Text Right Column Image</h2>
          <div class="w45" style="float: right">
            <img src="assets/berger-2009-f1_bursts.png">
            <p class="lcred"><a href="http://dx.doi.org10.1088/0004-637X/695/1/310">Berger+
            (2009)</a></p>
          </div>
          <p>Example text <em>with emphasis.</em></p>
          <p>Some math follows</p>
          <p>$$\nu_\textrm{cyc} = \frac{e B}{2 \pi m_e c}$$</p>
           <div class='footer'>
              * Spelke, Elizabeth S. "Principles of object perception." Cognitive science 14, no. 1 (1990): 29-56.
           </div>
        </section>
        
        <section>
          <h2>Point Cloud Viewer Slide</h2>
          <div align="center">
            <iframe   src="http://pointclouds.org/assets/viewer/pcl_viewer.html?load=http://www.jeremiepapon.com/wp_skeleton/content/uploads/centroids_000050.pcd" width="800" height="600" marginwidth="0" marginheight="0" frameborder="no" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="" style="max-width: 100%;"></iframe>
          </div>
        </section>
        
        <section>
          <h2>Full Width image slide</h2>
          <p><em>Dynamo</em>: converts mechanical to electromagnetic energy.</p>
          <div class="ctr w90">
            <img src="assets/wp-Sun_poster.svg">
            <p class="lcred"><a href="http://commons.wikimedia.org/wiki/File:Sun_poster.svg">Wikimedia Commons</a></p>
          </div>
          <p>Solar dynamo believed to be an “α–Ω” mechanism.</p>
        </section>

        <section>
          <h2>Video Slide</h2>
          <p>Some text about a video</p>
          <div class="ctr w80">
            <video src="assets/soho-C2_Apr01.webm" width="100%" muted loop controls></video>
            <p class="lcred">NASA <a href="http://sohowww.nascom.nasa.gov/gallery/Movies/flares.html">Solar and Heliospheric Observatory</a></p>
          </div>
        </section>

        <section>
          <h2> Dual Column Image slide</h2>
          <p>Some text.</p>
          <br>
          <div class="w45" style="display: inline-block; height: 360px; margin-right: 5px;">
            <img src="assets/728501main_themis-magnetosphere.jpg">
            <p class="lcred">NASA / Goddard Space Flight Center</p>
          </div>
          <div class="w45" style="display: inline-block; height: 360px; margin-left: 5px;">
            <video src="assets/nasa-earth-recut-yt360.webm" muted controls></video>
            <p class="lcred">NASA — <a href="http://youtu.be/lxWBlJ1kB7Q">YouTube</a></p>
          </div>
          <p class="fragment highlight-green"> This text will get highlighted!</p>
        </section>

    <!-- End of slides. -->

    <script src="reveal/lib/js/head.min.js"></script>
    <script src="reveal/js/reveal.min.js"></script>
    <script src="pdfjs/compatibility.js"></script>
    <script src="pdfjs/pdf.js"></script>

    <script>
      Reveal.initialize({
        controls: false,
        progress: true,
        history: true,
        center: true,
        keyboard: true,
        overview: true,

        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'default', // none/fade/slide/convex/concave/zoom
        transitionSpeed: 'default', // default/fast/slow

        math: {
          mathjax: 'mathjax/MathJax.js',
          config: 'TeX-AMS_HTML-full',
        },

        dependencies: [
          { src: 'reveal/lib/js/classList.js',
            condition: function() { return !document.body.classList; }},
          { src: 'reveal/plugin/markdown/marked.js',
            condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal/plugin/markdown/markdown.js',
            condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal/plugin/highlight/highlight.js', async: true,
            callback: function() { hljs.initHighlightingOnLoad (); }},
          { src: 'reveal/plugin/notes/notes.js', async: true,
            condition: function() { return !!document.body.classList; }},
          { src: 'mymath.js', async: true },
          { src: 'pdfimgs.js', async: true },
          { src: 'slideautostart.js', async: true },
        ],
      });

      // Only load SDO Data page if we go to that slide
      Reveal.addEventListener ('sdodataslide', function () {
        console.log('sdodataslide');
        document.querySelector ('#sdoiframe').src = 'http://sdo.gsfc.nasa.gov/data/'
      });
    </script>
  </body>
</html>
