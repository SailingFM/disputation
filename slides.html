<!doctype html>
<html lang="en">
  <head>
    <link href="captionss/captionss.css" rel="stylesheet" type="text/css">
    <meta charset="utf-8">
    <title>Perceptual Segmentation of Visual Streams by Tracking of Objects and Parts</title>
    <meta name="author" content="Jérémie Papon">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <link rel="stylesheet" href="reveal/css/reveal.min.css">
    <link rel="stylesheet" href="pkgwtheme.css" id="theme">
    <link rel="stylesheet" href="reveal/lib/css/zenburn.css">
    <script>
      document.write('<link rel="stylesheet" href="reveal/css/print/'+(window.location.search.match(/print-pdf/gi) ? 'pdf' : 'paper')+'.css" type="text/css" media="print">');
    </script>
    <!--[if lt IE 9]>
    <script src="reveal/lib/js/html5shiv.js"></script>
    <![endif]-->
  </head>

  <body>
    <div class="reveal">
      <div class="slides">
        <section>
          <p style="text-align: center"><br><br><br>Turn off the popup blocker before starting!</p>
          <p style="text-align: center">Open presenter console with <code>s</code>.</p>
          <p style="text-align: center">This really isn’t going to work if the assets haven't been downloaded!</p>
        </section>

        
        <section data-background="data/Buildings_Cropped_Merge.png">
          <h1>Perceptual Segmentation of Visual Streams</h1>
          <h3>by Tracking of Objects and Parts</h3>
          <footer style="font-size: 36px">             
          <a href="http://www.jeremiepapon.com/">Jérémie Papon</a><br>
          Georg-August-Universität Göttingen <br>
          Institut für Informatik <br>
          Göttingen, 2014 Oct 17
          </footer>
          <br>
          <p style="text-align: center"> Disputation for the award of the degree "Doctor of Philosophy" </p>
          
        </section>

        <!-- MOTIVATION -->
        <section>
          <h2>How do we learn to perceive objects?</h2>
          <blockquote>“Infants appear to perceive objects by analyzing three-dimensional surface arrangements and motions... [they] divide perceptual arrays into units that move as connected wholes, that move separately from one another, that tend to maintain their size and shape over motion, and that tend to act upon each other only on contact.” *</blockquote>
          <div class="fragment">
            <div class="w50" style="display: inline-block">
            <p>There are multiple interacting elements essential to development:</p>
              <ul>
                <li class="fragment">Coherent motion at multiple levels</li>
                <li class="fragment">Temporal continuity of size and shape</li>
                <li class="fragment">Only interact with contact</li>
              </ul>
            </div>
            <div class="w45" style="float: right">
              <img src="data/child_colors.jpg">
                
            </div>
          </div>
           <div class='footer'>
              * Spelke, Elizabeth S. "Principles of object perception." Cognitive science 14, no. 1 (1990): 29-56.
            </div>
        </section>  
        
        <section>
          <h2>Observations have no semantic meaning.</h2>
          <p>How can we create meaningful spatial and temporal partitioning of discrete sensor observations?</p>
          <video src="" width="100%" controls loop class="slideautostart"></video>
          <p class="lcred">TODO A Video of Human Manipulation</p>
        </section>

        <!-- Existing Stuff -->
        <section>
          <h2>Parsing Video Streams -<br> Existing Methodologies</h2>
          <p><strong>Video Object Segmentation</strong> 
              e.g.Abramov et al.<cite></cite>
              Grundmann et al.<cite></cite></p>
          <p>This parses a video into spatio-temporal volumes - “objects”</p>
          
          <video src="" height="50%" controls loop class="slideautostart"></video>
          <p class="lcred">TODO A Video of Segmentation.</p>
          
          <div class="fragment">This means “objects” must form <span class="fragment highlight-green">continuous spatio-temporal</span> volumes!</div>
          <div class='citation'>
            <footl>
              <footi>Abramov et al., <a href="http://dx.doi.org/10.1109/TCSVT.2012.2199389">Real-Time Segmentation of Stereo Videos on a Portable System With a Mobile GPU,</a> <em>IEEE Transactions on Circuits and Systems for Video Technology </em>2012.</footi>
              <footi>Grundmann et al., <a href="http://www.cc.gatech.edu/cpl/projects/videosegmentation/cvpr2010_videosegmentation.pdf">Efficient Hierarchical Graph Based Video Segmentation,</a><em>Computer Vision and Pattern Recognition (CVPR)</em> 2010.</footi>
            </footl>
          </div>
        </section>
        
        <section>
          <h2>Parsing Video Streams -<br> Existing Methodologies</h2>
          <p><strong>Semantic Event Chains</strong><cite></cite> - Represents by analyzing creation & deletion of edges in segment adjacency graph.</p>

          <p>Analysis of temporal evolution of graph structure yields semantics</p>
          <video src="" height="50%" controls loop class="slideautostart"></video>
          <p class="lcred">TODO A Video of Graph Evolution.</p>
          <div class="fragment">This requires  <span class="fragment highlight-green">a-priori knowledge</span> of objects!</div>
          <div class='citation'>
            <footl>
              <footi>Aksoy, Eren Erdal, et al. <a href="http://www.dpi.physik.uni-goettingen.de/~eaksoye/papers/IJRR_2011.pdf">Learning the semantics of object–action relations by observation.</a> <em>The International Journal of Robotics Research</em> (2011).</footi>
            </footl>
          </div>
        </section>
        
        <!-- MOVING TO 3D -->
        <section style="line-height: 135%">
          <h2>Transitioning to 3D</h2>
          <p>To Summarize most vexing issues </p>
          <ul>
            <li class="fragment">Segment into spatio-temporal volumes - <span class="fragment current-visible">cannot handle occlusions</span></li>
            <li class="fragment">Divide the scene into objects <em><span class="fragment highlight-green">before</span></em> observations.</li>
            <ul style="list-style-type: none"><li class="fragment">Cannot learn “object-ness” from observations</li></ul>
            <li class="fragment"> Only use color - 3D geometry is not considered </li>
          </ul>
          <p class="fragment">To overcome some of these, we use RGB-D sensors to capture Point Clouds</p>
          <div class="ctr w70">
          <figure class="embed hide-smooth dark" >
            <img src="data/openni_cams.jpg">
            <figcaption style="font-size:0.75em">
              Some OpenNI Sensors which capture RGB+D data.
            </figcaption>
          </figure>
          </div>
          <p class="rcred"><a href="http://www.pointclouds.org/">Point Cloud Library (PCL)</a></p>
        </section>  
        
        <section>
          <h2>A Point Cloud</h2>
          <div align="center">
            <iframe   src="http://pointclouds.org/assets/viewer/pcl_viewer.html?load=http://www.jeremiepapon.com/wp_skeleton/content/uploads/centroids_000050.pcd" width="800" height="500" marginwidth="0" marginheight="0" frameborder="no" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="" style="max-width: 100%;">
            </iframe>
          </div>
          <p>Advantages of 3D</p>
          <ul>
            <li class="fragment"> TODO Something. </li>
            <li class="fragment"> TODO Something. </li>
            <li class="fragment"> TODO Something. </li>
          </ul>
        </section>
        
        <!-- SUPERVOXELS -->
        <section>
          <h2>Octree Voxelization</h2>
          <div class="w45" style="float: right">
            <img src="data/bunnywork.png">
            <p class="rcred"><a href="http://www.pointclouds.org/">Point Cloud Library (PCL)</a></p>
          </div>
          <p>Insert pointcloud into a grid of cubic voxels.</p>
          <p>Represent all points in one cell by its centroid.</p>
          <p>$$\vec{ \overline{p}} = \frac{1}{N_i}\sum_i \vec p_i$$</p>
          <p>The edge length $L_\text{voxel}$ of voxels defines scale of observation and determines octree minimum bin size.</p>
        </section>
        
        <section>
          <h2>Building an Adjacency Graph</h2>
            <ul>
              <li class="fragment">Special octree type developed which maintains adjacency information of voxels</li>
              <li class="fragment">This gives us back pixel-like (grid) relations, while keeping real 3D adjacency</li>
              <li class="fragment">Region growing and connectivity graph become very efficient</li>
            </ul>
          <div class="ctr w90">
            <figure class="embed hide-smooth dark" >
              <img src="">
              <figcaption style="font-size:1.0em">
                TODO Adjacency Octree - Graph structure from point clouds.
              </figcaption>
            </figure>
          </div>
        </section>
        
        <section>
          <h2>Voxel Cloud Connectivity Segmentation</h2>
          <div class="ctr w100">
            <img src="data/supervoxelwhite.svg" width="75%">
          </div>
          <ul>
            <li class="fragment">VCCS <cite></cite> is an oversegmentation technique that uses local geometry to respect object boundaries</li>
            <li class="fragment">Constrained to flow across voxel connections</li>
            <li class="fragment">Use color, normals, and a spatial smoothness constraint </li>
          </ul>
          <div class="ctr w100">
            <img src="data/Supervoxels4.svg" width="80%">
          </div>

           <div class='citation'>
            <footl>
              <footi>Papon et al., <a href="http://www.jeremiepapon.com/cvpr-2013-supervoxels/">Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds,</a> Computer Vision and Pattern Recognition (CVPR) 2013.</footi>
            </footl>
          </div>
        </section>
  
        <section>
          <h2>Examples of Supervoxels</h2>
          <div class="ctr w100">
            <img src="data/IncreasingSeedSizePlain.svg" width="100%" >
            <p class="rcred"><a href="http://www.jeremiepapon.com/cvpr-2013-supervoxels/">Papon et al. CVPR 2013</a></p>
          </div>
          <p> Example of Supervoxels with different seed sizes - from NYU Dataset <cite></cite> </p>
          <p> Seeding size determines level of detail captured.</p>
          
          <div class='citation'>
            <footl>
              <footi>Silberman et al., <a href="http://cs.nyu.edu/~silberman/projects/indoor_scene_seg_sup.html">Indoor Segmentation and Support Inference from RGBD Images,</a> European Conference on Computer Vision (ECCV) 2012.</footi>
            </footl>
          </div>
        </section>
        
        <section>
          <h2>Supervoxels in a Point Cloud</h2>
          <iframe   src="http://pointclouds.org/assets/viewer/pcl_viewer.html?load=http://www.jeremiepapon.com/wp_skeleton/content/uploads/centroids_000050.pcd" width="800" height="500" marginwidth="0" marginheight="0" frameborder="no" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="" style="max-width: 100%;">
          </iframe>
        </section>
        
        <section>
          <h2>Local Convexity Segmentation</h2>
          <p>Can get a segmentation into parts by breaking graph edges using convexity.</p>
          <div class="ctr w90">
          <figure class="embed hide-smooth dark" >
            <img src="">
            <figcaption style="font-size:1.0em">
              TODO Hierarchy - voxels to supervoxels to connected convex segments.
            </figcaption>
          </figure>
          </div>
        </section>
        
        <section> 
          <h2> Can segment huge full 3D scenes efficiently. </h2>
          <p> TODO Add City Segmentation! XYZRGBL! </p>
          <iframe   src="http://pointclouds.org/assets/viewer/pcl_viewer.html?load=http://www.jeremiepapon.com/wp_skeleton/content/uploads/centroids_000050.pcd" width="800" height="500" marginwidth="0" marginheight="0" frameborder="no" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="" style="max-width: 100%;">
            </iframe>
        </section>
        
        <!-- SEQUENTIAL POINT CLOUDS -->
        <section>
          <h2>Sequential Clouds & Occlusion Reasoning </h2>
          <p> Occlusions appear as “shadows” in rendered point clouds. </p>
          <video src="" height="50%" controls loop class="slideautostart"></video>
          <p class="lcred">TODO Video of covering lemon without occlusion reasoning.</p>
          <p> As new observations arrive, we can begin low-level reasoning about occlusion</p>
        </section>
        
        <section>
          <h2>Sequentially Updated Octree <cite></cite> </h2>
          <p> If we assume no camera motion, we can reason about why voxels “disappear” </p>
          <p> Check for occlusion by ray-tracing paths from voxel to camera</p>
          <video src="" height="50%" controls loop class="slideautostart"></video>
          <p class="lcred">TODO Video of covering lemon with occlusion reasoning.</p>
          
          <div class='citation'>
            <footl>
              <footi>Papon et al., <a href="http://www.jeremiepapon.com/iros-2013-video-segmentation/">Point Cloud Video Object Segmentation using a Persistent Supervoxel World-Model,</a> Intelligent Robots and Systems (IROS) 2013.</footi>
            </footl>
          </div>
        </section>
        
        <!-- PARTICLE FILTERING -->
        <section>
          <h2>Particle filter tracking in Point Clouds</h2>
          <p> Model Representation </p>
        </section>
        
        <section>
          <h2>Particle filter tracking in Point Clouds</h2>
          <p> Correspondence Matching </p>
        </section>
        
        <section>
          <h2>Stratified Correspondence Sampling <cite></cite></h2>
          
          <div class='citation'>
            <footl>
              <footi>Papon et al., <a href="http://www.jeremiepapon.com/wacv-2015-tracking/">Spatially Stratified Correspondence Sampling for Real-Time Point Cloud Tracking,</a> Applications of Computer Vision (WACV), 2015.</footi>
            </footl>
          </div>
        </section>
        
        <section>
          <h2>Results on VR Data</h2>
          <p> TODO Graph showing performance. </p>
          
        </section>
        
        <section>
          <h2> Results on VR Data </h2>
          <p> TODO Video Results on Tide Sequence </p>
        </section>
        
        <!-- HIERARCHICAL SUPERVOXEL TRACKING -->
        <section>
          <h2> What was our purpose again? </h2>
          <p class="fragment"> To understand a video like this! </p>
          <p> TODO Show cutting video with model tracking - failure!</p>
          <p class="fragment"> Unfortunately, this is all we have so far with our occlusion reasoning and particle filtering.</p>
          <p class="fragment"> This can only be resolved by eliminating the a-priori object models.</p>
          
        </section>
        
        <section>
          <h2> Tracking Supervoxels? </h2>
          <p> We cannot track at the supervoxel level due to the “aperture problem” <cite></cite></p>
          <embed src="data/flash/two-squares.swf" width="90%" height="80%"></embed>
          <p class="rcred"><a href="http://web.mit.edu/persci/demos/Motion&Form/demos/download.html">MIT Perceptual Science Group</a></p>
          <div class='citation'>
            <footl>
              <footi>McDermott, et al., <a href="http://web.mit.edu/jhm/www/Pubs/McDermott_Weiss_Adelson_2001_motion_form_beyond_junctions.pdf">Beyond junctions: Nonlocal form contraints on motion interpretation.</a> <em>Perception </em> 2001.</footi>
            </footl>
          </div>
        </section>
        
        <section>
          <h2> Hierarchical Temporal Supervoxels </h2>
          <div class="ctr w90">
          <figure class="embed hide-smooth dark" >
            <img src="">
            <figcaption style="font-size:1.0em">
              TODO Big figure showing hierarchical update - white board!
            </figcaption>
          </figure>
          </div>
        </section>
        
        <section>
          <h2> Hierarchical Temporal Supervoxels </h2>
          <p> Human Manipulation Video </p>
        </section>
        
        <section>
          <h2> Hierarchical Temporal Supervoxels </h2>
          <p> Cutting Video </p>
        </section>
        
        <section>
          <h2> Hierarchical Temporal Supervoxels </h2>
          <p> Occlusion Manip 1 Video </p>
        </section>
        
        <section>
          <h2> Hierarchical Temporal Supervoxels </h2>
          <p> Pan Video </p>
        </section>
        
        <section>
          <h2>Summary</h2>
        </section>
        
        <section>
          <h2>Outlook and Future Work</h2>
        </section>
        
        <section>
          <h2>Other Contributions</h2>
          <p> Oculus vision GUI </p>
          <p> All algorithms have been released as Open Source </p>
          <p> 2D Tracking and Segmentation using Particle Filters </p>
        </section>
        
        <section>
          <h2>Acknowledgements and Thanks</h2>
          <h4 align="left">Thesis-related Publications</h4>
          <ul style="font-size: 60%; line-height: 135%; width:50%; float:left">
            <li><strong>Papon, J.</strong>;  Wörgötter, F., <a href="http://www.jeremiepapon.com/wacv-2015-tracking/">Spatially Stratified Correspondence Sampling for Real-Time Point Cloud Tracking,</a> Applications of Computer Vision (WACV), 2015 IEEE International Conference on, Jan. 2015.</li>
            <br style="line-height:150%">
            <li>Stein, S.; Schoeler, M.; <strong>Papon, J.</strong>;  Wörgötter, F., <a href="http://www.jeremiepapon.com/cvpr-2014-segmentation/">Object Partitioning using Local Convexity,</a> Computer Vision and Pattern Recognition (CVPR) 2014, June 2014. </li>
            <br style="line-height:150%">
            <li><strong>Papon, J.</strong>;  Kulvicius, T.; Aksoy, E.; Wörgötter, F. <a href="http://www.jeremiepapon.com/iros-2013-video-segmentation/">Point Cloud Video Object Segmentation using a Persistent Supervoxel World-Model,</a> Intelligent Robots and Systems (IROS), 2013 IEEE/RSJ International Conference on, Nov. 2013.</li>
            <br style="line-height:150%">
            <li><strong>Papon, J.</strong>; Abramov, A.; Schoeler, M.; Wörgötter, F., <a href="http://www.jeremiepapon.com/cvpr-2013-supervoxels/">Voxel Cloud Connectivity Segmentation - Supervoxels for Point Clouds,</a> Computer Vision and Pattern Recognition (CVPR) 2013, June 2013.</li>
            <br style="line-height:150%">
            <li><strong>Papon, J.</strong>; Abramov, A.; Wörgötter, F., <a href="http://www.jeremiepapon.com/eccv-2012-ws-2d-video-segmentation/">Occlusion Handling in Video Segmentation via Predictive Feedback,</a> European Conference on Computer Vision (ECCV) 2012, Workshops and Demonstrations, Oct. 2012.</li>
            <br style="line-height:150%">
            <li><strong>Papon, J.</strong>; Abramov, A.; Aksoy, E.; Wörgötter, F., <a href="http://www.jeremiepapon.com/wacv-2012-oculus-system/">A modular system architecture for online parallel vision pipelines,</a> Applications of Computer Vision (WACV) 2012, Jan. 2012.</li>
          </ul>
          <div style="width:40%; float:right">
            <div style="width:30%; float:left">
            <figure class="embed hide-smooth dark" >
                  <img src="data/hogrefe.jpg" alt="Dieter Hogrefe">
                  <figcaption style="font-size:0.5em">
                      Dieter Hogrefe
                  </figcaption>
              </figure>
            </div>
            <div style="width:35%; float:right">
              <figure class="embed hide-smooth dark" >
                  <img src="data/worgotter.png" alt="Florentin Wörgötter">
                  <figcaption style="font-size:0.5em">
                      Florentin Wörgötter
                  </figcaption>
              </figure>
            </div>
          
          <img src="data/point-cloud-library-logo.png">
          </div>
        </section>
        
        <section>
          <img src="data/thesis_defense.png" height="90%">
          <p class="rcred"><a href="http://xkcd.com/1403/">XKCD</a></p>
        </section>
        
        <section>
          <h2>Left Column Text Right Column Image</h2>
          <div class="w45" style="float: right">
            <img src="assets/berger-2009-f1_bursts.png">
            <p class="lcred"><a href="http://dx.doi.org10.1088/0004-637X/695/1/310">Berger+
            (2009)</a></p>
          </div>
          <p>Example text <em>with emphasis.</em></p>
          <p>Some math follows</p>
          <p>$$\nu_\textrm{cyc} = \frac{e B}{2 \pi m_e c}$$</p>
           <div class='footer'>
              * Spelke, Elizabeth S. "Principles of object perception." Cognitive science 14, no. 1 (1990): 29-56.
           </div>
        </section>
        
        <section>
          <h2>Point Cloud Viewer Slide</h2>
          <div align="center">
            <iframe   src="http://pointclouds.org/assets/viewer/pcl_viewer.html?load=http://www.jeremiepapon.com/wp_skeleton/content/uploads/centroids_000050.pcd" width="800" height="600" marginwidth="0" marginheight="0" frameborder="no" allowfullscreen="" mozallowfullscreen="" webkitallowfullscreen="" style="max-width: 100%;"></iframe>
          </div>
        </section>
        
        <section>
          <h2>Full Width image slide</h2>
          <p><em>Dynamo</em>: converts mechanical to electromagnetic energy.</p>
          <div class="ctr w90">
            <img src="assets/wp-Sun_poster.svg">
            <p class="lcred"><a href="http://commons.wikimedia.org/wiki/File:Sun_poster.svg">Wikimedia Commons</a></p>
          </div>
          <p>Solar dynamo believed to be an “α–Ω” mechanism.</p>
        </section>

        <section>
          <h2>Video Slide</h2>
          <p>Some text about a video</p>
          <div class="ctr w80">
            <video src="assets/soho-C2_Apr01.webm" width="100%" muted loop controls></video>
            <p class="lcred">NASA <a href="http://sohowww.nascom.nasa.gov/gallery/Movies/flares.html">Solar and Heliospheric Observatory</a></p>
          </div>
        </section>

        <section>
          <h2> Dual Column Image slide</h2>
          <p>Some text.</p>
          <br>
          <div class="w45" style="display: inline-block; height: 360px; margin-right: 5px;">
            <img src="assets/728501main_themis-magnetosphere.jpg">
            <p class="lcred">NASA / Goddard Space Flight Center</p>
          </div>
          <div class="w45" style="display: inline-block; height: 360px; margin-left: 5px;">
            <video src="assets/nasa-earth-recut-yt360.webm" muted controls></video>
            <p class="lcred">NASA — <a href="http://youtu.be/lxWBlJ1kB7Q">YouTube</a></p>
          </div>
          <p class="fragment highlight-green"> This text will get highlighted!</p>
        </section>

    <!-- End of slides. -->

    <script src="reveal/lib/js/head.min.js"></script>
    <script src="reveal/js/reveal.min.js"></script>
    <script src="pdfjs/compatibility.js"></script>
    <script src="pdfjs/pdf.js"></script>

    <script>
      Reveal.initialize({
        width: 1024,
        height: 768,
        controls: false,
        progress: true,
        history: true,
        center: true,
        keyboard: true,
        overview: true,

        theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
        transition: Reveal.getQueryHash().transition || 'default', // none/fade/slide/convex/concave/zoom
        transitionSpeed: 'default', // default/fast/slow

        math: {
          mathjax: 'mathjax/MathJax.js',
          config: 'TeX-AMS_HTML-full',
        },

        dependencies: [
          { src: 'reveal/lib/js/classList.js',
            condition: function() { return !document.body.classList; }},
          { src: 'reveal/plugin/markdown/marked.js',
            condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal/plugin/markdown/markdown.js',
            condition: function() { return !!document.querySelector ('[data-markdown]'); }},
          { src: 'reveal/plugin/highlight/highlight.js', async: true,
            callback: function() { hljs.initHighlightingOnLoad (); }},
          { src: 'reveal/plugin/notes/notes.js', async: true,
            condition: function() { return !!document.body.classList; }},
          { src: 'mymath.js', async: true },
          { src: 'pdfimgs.js', async: true },
          { src: 'slideautostart.js', async: true },
        ],
      });

      // Only load SDO Data page if we go to that slide
      Reveal.addEventListener ('sdodataslide', function () {
        console.log('sdodataslide');
        document.querySelector ('#sdoiframe').src = 'http://sdo.gsfc.nasa.gov/data/'
      });
    </script>
  </body>
</html>
